{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stats.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMPcgQKaP3lCzGSZWrZE16",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmsaravanan99/Major-Project/blob/main/stats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_plsqtrBSvjt"
      },
      "source": [
        "#@uthor: Ayush Pareek\n",
        "import sys, time, os\n",
        "import random, re, csv, collections\n",
        "import nltk, pylab, numpy\n",
        "\n",
        "import preprocessing\n",
        "\n",
        "def printClassStats( tweets ):\n",
        "    tweets_counter = collections.Counter( [t[1] for t in tweets] )\n",
        "    print '%8s %8s %s' % ('Class', 'Count', 'Example')\n",
        "    for (sent, count) in tweets_counter.items():\n",
        "        print '%8s %8d %s' % (sent, count, random.choice([t for (t,s,_,_) in tweets if s==sent ]) )\n",
        "\n",
        "def printFeaturesStats( tweets ):\n",
        "    arr_Handles   = numpy.array( [0]*len(tweets) )\n",
        "    arr_Hashtags  = numpy.array( [0]*len(tweets) )\n",
        "    arr_Urls      = numpy.array( [0]*len(tweets) )\n",
        "    arr_Emoticons = numpy.array( [0]*len(tweets) )\n",
        "    arr_Words     = numpy.array( [0]*len(tweets) )\n",
        "    arr_Chars     = numpy.array( [0]*len(tweets) )\n",
        "    \n",
        "\n",
        "    i=0\n",
        "    for (text, sent, subj, quer) in tweets:\n",
        "        arr_Handles[i]   = preprocessing.countHandles(text)\n",
        "        arr_Hashtags[i]  = preprocessing.countHashtags(text)\n",
        "        arr_Urls[i]      = preprocessing.countUrls(text)\n",
        "        arr_Emoticons[i] = preprocessing.countEmoticons(text)\n",
        "        arr_Words[i]     = len(text.split())\n",
        "        arr_Chars[i]     = len(text)\n",
        "        i+=1\n",
        "\n",
        "    print '%-10s %-010s %-4s '%('Features',  'Average',            'Maximum')\n",
        "    print '%10s %10.6f %10d'%('Handles',   arr_Handles.mean(),   arr_Handles.max()   )\n",
        "    print '%10s %10.6f %10d'%('Hashtags',  arr_Hashtags.mean(),  arr_Hashtags.max()  )\n",
        "    print '%10s %10.6f %10d'%('Urls',      arr_Urls.mean(),      arr_Urls.max()      )\n",
        "    print '%10s %10.6f %10d'%('Emoticons', arr_Emoticons.mean(), arr_Emoticons.max() )\n",
        "    print '%10s %10.6f %10d'%('Words',     arr_Words.mean(),     arr_Words.max()     )\n",
        "    print '%10s %10.6f %10d'%('Chars',     arr_Chars.mean(),     arr_Chars.max()     )\n",
        "\n",
        "def printReductionStats( tweets, function, filtering=True):\n",
        "    if( function ):\n",
        "        procTweets = [ (function(text, subject=subj, query=quer), sent)    \\\n",
        "                        for (text, sent, subj, quer) in tweets]\n",
        "    else:\n",
        "        procTweets = [ (text, sent)    \\\n",
        "                        for (text, sent, subj, quer) in tweets]\n",
        "    tweetsArr = []\n",
        "    for (text, sentiment) in procTweets:\n",
        "        words = [word if(word[0:2]=='__') else word.lower() \\\n",
        "                        for word in text.split() \\\n",
        "                        if ( (not filtering) | (len(word) >= 3) ) ]\n",
        "        tweetsArr.append([words, sentiment])\n",
        "    # tweetsArr\n",
        "    bag = collections.Counter()\n",
        "    for (words, sentiment) in tweetsArr:\n",
        "        bag.update(words)\n",
        "    # unigram\n",
        "\n",
        "    print '%20s %-10s %12d'% (\n",
        "                ('None' if function is None else function.__name__),\n",
        "                ( 'gte3' if filtering else 'all' ),\n",
        "                sum(bag.values())\n",
        "            )\n",
        "    return True\n",
        "\n",
        "def printAllRecuctionStats(tweets):\n",
        "    print '%-20s %-10s %-12s'% ( 'Preprocessing', 'Filter', 'Words' )\n",
        "    printReductionStats( tweets, None,                   False   )\n",
        "    #printReductionStats( tweets, None,                   True    )\n",
        "    printReductionStats( tweets, preprocessing.processHashtags,        True    )\n",
        "    printReductionStats( tweets, preprocessing.processHandles,         True    )\n",
        "    printReductionStats( tweets, preprocessing.processUrls,            True    )\n",
        "    printReductionStats( tweets, preprocessing.processEmoticons,       True    )\n",
        "    printReductionStats( tweets, preprocessing.processPunctuations,    True    )\n",
        "    printReductionStats( tweets, preprocessing.processRepeatings,      True    )\n",
        "    #printReductionStats( tweets, preprocessing.processAll,             False   )\n",
        "    printReductionStats( tweets, preprocessing.processAll,             True    )\n",
        "\n",
        "def printFreqDistCSV( dist, filename='' ):\n",
        "    n_samples = len(dist.keys())\n",
        "    n_repeating_samples = sum([ 1 for (k,v) in dist.items\n",
        "        () if v>1 ])\n",
        "    n_outcomes = dist._N\n",
        "    print '%-12s %-12s %-12s'%( 'Samples', 'RepSamples', 'Outcomes' )\n",
        "    print '%12d %12d %12d'%( n_samples, n_repeating_samples, n_outcomes )\n",
        "    \n",
        "    if( len(filename)>0 and '_'!=filename[0] ):\n",
        "        with open( filename, 'w' ) as fcsv:\n",
        "            distwriter = csv.writer( fcsv, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC )\n",
        "            \n",
        "            for (key,value) in dist.items():\n",
        "                distwriter.writerow( [key, value] ) #print key, '\\t,\\t', dist[key]\n",
        "\n",
        "def preprocessingStats( tweets, fileprefix='' ):\n",
        "\n",
        "    if( len(fileprefix)>0 and '_'!=fileprefix[0] ):\n",
        "        directory = os.path.dirname(fileprefix)\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "        print 'writing to', fileprefix+'_stats.txt'\n",
        "        realstdout = sys.stdout\n",
        "        sys.stdout = open( fileprefix+'_stats.txt' , 'w')\n",
        "\n",
        "    ###########################################################################  \n",
        "\n",
        "    print 'for', len(tweets), 'tweets:'\n",
        "\n",
        "    print '###########################################################################'\n",
        "\n",
        "    printFeaturesStats( tweets )\n",
        "\n",
        "    print '###########################################################################'\n",
        "\n",
        "    printAllRecuctionStats( tweets )\n",
        "\n",
        "    print '###########################################################################'\n",
        "\n",
        "    procTweets = [ (preprocessing.processAll(text, subject=subj, query=quer), sent)    \\\n",
        "                        for (text, sent, subj, quer) in tweets]\n",
        "    tweetsArr = []\n",
        "    for (text, sentiment) in procTweets:\n",
        "        words = [word if(word[0:2]=='__') else word.lower() \\\n",
        "                        for word in text.split() \\\n",
        "                        if ( (len(word) >= 3) ) ]\n",
        "        tweetsArr.append([words, sentiment])\n",
        "    unigrams_fd = nltk.FreqDist()\n",
        "    bigrams_fd = nltk.FreqDist()\n",
        "    trigrams_fd = nltk.FreqDist()\n",
        "    for (words, sentiment) in tweetsArr:\n",
        "        words_bi = [ ','.join(map(str,bg)) for bg in nltk.bigrams(words) ]\n",
        "        words_tri  = [ ','.join(map(str,tg)) for tg in nltk.trigrams(words) ]\n",
        "        unigrams_fd.update( words )\n",
        "        bigrams_fd.update( words_bi )\n",
        "        trigrams_fd.update( words_tri )\n",
        "\n",
        "    print 'Unigrams Distribution'\n",
        "    printFreqDistCSV(unigrams_fd, filename=fileprefix+'_1grams.csv')\n",
        "    if( len(fileprefix)>0 and '_'!=fileprefix[0] ):\n",
        "        pylab.show = lambda : pylab.savefig(fileprefix+'_1grams.pdf')\n",
        "    unigrams_fd.plot(50, cumulative=True)\n",
        "    pylab.close()\n",
        "\n",
        "    print 'Bigrams Distribution'\n",
        "    printFreqDistCSV(bigrams_fd, filename=fileprefix+'_2grams.csv')\n",
        "    if( len(fileprefix)>0 and '_'!=fileprefix[0] ):\n",
        "        pylab.show = lambda : pylab.savefig(fileprefix+'_2grams.pdf')\n",
        "    bigrams_fd.plot(50, cumulative=True)\n",
        "    pylab.close()\n",
        "\n",
        "    print 'Trigrams Distribution'\n",
        "    printFreqDistCSV(trigrams_fd, filename=fileprefix+'_3grams.csv')\n",
        "    if( len(fileprefix)>0 and '_'!=fileprefix[0] ):\n",
        "        pylab.show = lambda : pylab.savefig(fileprefix+'_3grams.pdf')\n",
        "    trigrams_fd.plot(50, cumulative=True)\n",
        "    pylab.close()\n",
        "\n",
        "    if( len(fileprefix)>0 and '_'!=fileprefix[0] ):\n",
        "        pylab.show = lambda : pylab.savefig(fileprefix+'_ngrams.pdf')\n",
        "    unigrams_fd.plot(50, cumulative=True)\n",
        "    bigrams_fd.plot(50, cumulative=True)\n",
        "    trigrams_fd.plot(50, cumulative=True)\n",
        "    pylab.close()    \n",
        "\n",
        "    if( len(fileprefix)>0 and '_'!=fileprefix[0] ):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = realstdout\n",
        "\n",
        "def stepStats( tweets, num_bins=10, split='easy', fileprefix='' ):\n",
        "    tot_size = len(tweets)\n",
        "    num_digits = len(str(tot_size))\n",
        "\n",
        "    if split=='equal':\n",
        "        sizes = [ int((r+1.0)/num_bins*tot_size) for r in range( num_bins ) ]\n",
        "    elif split=='log':\n",
        "        sizes = [ int(2**(math.log(tot_size,2)*(r+1.0)/num_bins) ) for r in range( num_bins ) ]\n",
        "    else: # split=='easy'\n",
        "        sizes = range( 0, tot_size, tot_size/num_bins)[1:]+[tot_size]\n",
        "\n",
        "    for s in sizes:\n",
        "        print 'processing stats for %d tweets'%s\n",
        "        preprocessingStats( tweets[0:s], fileprefix+'_%0{0}d'.format(num_digits) % s )\n",
        "\n",
        "def oldStats2CSV( in_file, fileprefix=''):\n",
        "    if fileprefix == '':\n",
        "        fileprefix = in_file.rstrip('_stats.txt')\n",
        "    fp = open( in_file, 'r' )\n",
        "    fq = open( fileprefix+'_statsnew.txt', 'w' )\n",
        "\n",
        "    line = ''\n",
        "    line_start = 0\n",
        "    line_count = 20\n",
        "    line_end   = line_start+line_count\n",
        "    for line_num in range(line_start, line_end):   # write Statistics\n",
        "        line = fp.readline()\n",
        "        fq.write( line )\n",
        "\n",
        "    for section in [1,2,3]:\n",
        "        line_start = line_end\n",
        "        line_count = 2\n",
        "        line_end   = line_start+line_count\n",
        "        for line_num in range( line_start, line_end ):\n",
        "            line = fp.readline()\n",
        "            fq.write( line )\n",
        "    \n",
        "        line_start = line_end\n",
        "        line_count = [int(l) for l in line.split() if l.isdigit()][0]\n",
        "        line_end = line_start+line_count\n",
        "        fr = open( fileprefix+'_%dgrams.csv'%section, 'w')\n",
        "        fwrt = csv.writer( fr, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC )\n",
        "        for line_num in range( line_start, line_end ):  # write unigrams\n",
        "            line = fp.readline()\n",
        "            row = line.split('\\t,\\t')\n",
        "            row[0] = row[0].strip()\n",
        "            row[1] = int(row[1])\n",
        "            fwrt.writerow( row )\n",
        "        fr.close()\n",
        "\n",
        "    fp.close()\n",
        "    fq.close()\n",
        "\n",
        "stats_tiltes = [\n",
        "'\"# tweets\"',\n",
        "'\"avg(Handles)\"',\n",
        "'\"max(Handles)\"',\n",
        "'\"avg(Hashtags)\"',\n",
        "'\"max(Hashtags)\"',\n",
        "'\"avg(Urls)\"',\n",
        "'\"max(Urls)\"',\n",
        "'\"avg(Emoticons)\"',\n",
        "'\"max(Emoticons)\"',\n",
        "'\"avg(Words)\"',\n",
        "'\"max(Words)\"',\n",
        "'\"avg(Chars)\"',\n",
        "'\"max(Chars)\"',\n",
        "'\"preprocessing(None)\"',\n",
        "'\"preprocessing(Hashtags)\"',\n",
        "'\"preprocessing(Handles)\"',\n",
        "'\"preprocessing(Urls)\"',\n",
        "'\"preprocessing(Emoticons)\"',\n",
        "'\"preprocessing(Punctuations)\"',\n",
        "'\"preprocessing(Repeatings)\"',\n",
        "'\"preprocessing(All)\"',\n",
        "'\"Unigrams samples\"',\n",
        "'\"Unigrams repeating samples\"',\n",
        "'\"Unigrams outcomes\"',\n",
        "'\"Bigrams samples\"',\n",
        "'\"Bigrams repeating samples\"',\n",
        "'\"Bigrams outcomes\"',\n",
        "'\"Trigrams samples\"',\n",
        "'\"Trigrams repeating samples\"',\n",
        "'\"Trigrams outcomes\"',\n",
        "]\n",
        "\n",
        "def newStats2CSV(files, out_file):\n",
        "\n",
        "    arr = [ [] ] * len(files)\n",
        "\n",
        "    for j in range( len(files)):\n",
        "        values = []\n",
        "        with open(files[j], 'r') as fp:\n",
        "            for line in fp:\n",
        "                values += [ float(w) for w in line.split()\\\n",
        "                                if  w[0] in ['0','1','2','3','4','5','6','7','8','9'] ]\n",
        "        arr[j] = values\n",
        "\n",
        "    with open(out_file, 'w') as fq:\n",
        "        stats_writer = csv.writer( fq, delimiter=',', quotechar='\\'')#, quoting=csv.QUOTE_NONE )\n",
        "        for i in range(0,len(stats_tiltes)):\n",
        "            row = [stats_tiltes[i]] + [arr[j][i] for j in range(len(files))]\n",
        "            stats_writer.writerow( row )\n",
        "\n",
        "\n",
        "filelist = [\n",
        "'logs/stats_140617-214922-IST/Both_0978_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_1956_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_2934_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_3912_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_4890_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_5868_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_6846_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_7824_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_8802_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_9780_stats.txt',\n",
        "'logs/stats_140617-214922-IST/Both_9782_stats.txt',\n",
        "]\n",
        "\n",
        "\n",
        "['0','1','2','3','4','5','6','7','8','9']\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}