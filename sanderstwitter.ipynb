{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sanderstwitter.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMXnB6QBmdTl2YD/ynmMTEQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmsaravanan99/Major-Project/blob/main/sanderstwitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPQqyuMlPCVN"
      },
      "source": [
        "___init__.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P-5NrfFPajG"
      },
      "source": [
        "  \n",
        "import csv\n",
        "\n",
        "queryTerms = {\\\n",
        "                'apple'     : ['@apple',    ],  \\\n",
        "                'microsoft' : ['#microsoft', ], \\\n",
        "                'google'    : ['#google', ],    \\\n",
        "                'twitter'   : ['#twitter', ],    \\\n",
        "    }\n",
        "\n",
        "def getTweetsRawData( fileName ):\n",
        "    # read all tweets and labels\n",
        "    fp = open( fileName, 'rb' )\n",
        "    reader = csv.reader( fp, delimiter=',', quotechar='\"', escapechar='\\\\' )\n",
        "    tweets = []\n",
        "    for row in reader:\n",
        "        tweets.append( [row[4], row[1], row[0], queryTerms[(row[0]).lower()] ] )\n",
        "    # treat neutral and irrelevant the same\n",
        "    for t in tweets:\n",
        "        if (t[1] == 'positive'):\n",
        "            t[1] = 'pos'\n",
        "        elif (t[1] == 'negative'):\n",
        "            t[1] = 'neg'\n",
        "        elif (t[1] == 'irrelevant')|(t[1] == 'neutral'):\n",
        "            t[1] = 'neu'\n",
        "\n",
        "    return tweets # 0: Text # 1: class # 2: subject # 3: query\n",
        "\n",
        "SampleTweetsStats = '''\n",
        "   Class    Count Example\n",
        "     neg      529 #Skype often crashing: #microsoft, what are you doing?\n",
        "     neu     3770 How #Google Ventures Chooses Which Startups Get Its $200 Million http://t.co/FCWXoUd8 via @mashbusiness @mashable\n",
        "     pos      483 Now all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO91vXzbPhXy"
      },
      "source": [
        "install.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW5m-B3zPo2e"
      },
      "source": [
        "#\n",
        "# Sanders-Twitter Sentiment Corpus Install Script\n",
        "# Version 0.1\n",
        "#\n",
        "# Adapted from http://www.sananalytics.com/lab/twitter-sentiment/\n",
        "#\n",
        "#\n",
        "import csv, getpass, json, os, time, random\n",
        "\n",
        "# using python-twitter library\n",
        "import twitter\n",
        "\n",
        "\n",
        "def get_user_params():\n",
        "\n",
        "    user_params = {}\n",
        "\n",
        "    # get user input params\n",
        "    user_params['inList']  = '' #raw_input( '\\nInput file [./corpus.csv]: ' )\n",
        "    user_params['outList'] = '' #raw_input( 'Results file [./full-corpus.csv]: ' )\n",
        "    user_params['rawDir']  = '' #raw_input( 'Raw data dir [./rawdata/]: ' )\n",
        "\n",
        "    # apply defaults\n",
        "    if user_params['inList']  == '':\n",
        "        user_params['inList'] = './corpus.csv'\n",
        "    if user_params['outList'] == '':\n",
        "        user_params['outList'] = './full-corpus.csv'\n",
        "    if user_params['rawDir']  == '':\n",
        "        user_params['rawDir'] = './rawdata/'\n",
        "\n",
        "    return user_params\n",
        "\n",
        "\n",
        "def dump_user_params( user_params ):\n",
        "\n",
        "    # dump user params for confirmation\n",
        "    print 'Input:    '   + user_params['inList']\n",
        "    print 'Output:   '   + user_params['outList']\n",
        "    print 'Raw data: '   + user_params['rawDir']\n",
        "    return\n",
        "\n",
        "def filter_list( total_list ) :\n",
        "    # filtering only apple for test purposes\n",
        "    indices = [i for i in range( len( total_list ) ) if (total_list[i])[0] ==  \"apple\"]\n",
        "    return [total_list[i] for i in indices]\n",
        "\n",
        "def read_total_list( in_filename ):\n",
        "\n",
        "    # read total fetch list csv\n",
        "    fp = open( in_filename, 'rb' )\n",
        "    reader = csv.reader( fp, delimiter=',', quotechar='\"' )\n",
        "\n",
        "    total_list = []\n",
        "    for row in reader:\n",
        "        total_list.append( row )\n",
        "\n",
        "    return total_list\n",
        "\n",
        "\n",
        "def purge_already_fetched( fetch_list, raw_dir ):\n",
        "\n",
        "    # list of tweet ids that still need downloading\n",
        "    rem_list = []\n",
        "    count = 0;\n",
        "\n",
        "    # check each tweet to see if we have it\n",
        "    for item in fetch_list:\n",
        "\n",
        "        # check if json file exists\n",
        "        tweet_file = raw_dir + item[2] + '.json'\n",
        "        if os.path.exists( tweet_file ):\n",
        "\n",
        "            # attempt to parse json file\n",
        "            try:\n",
        "                parse_tweet_json( tweet_file )\n",
        "                count = count + 1\n",
        "                print '--> already downloaded #' + item[2]\n",
        "            except RuntimeError:\n",
        "                rem_list.append( item )\n",
        "        else:\n",
        "            rem_list.append( item )\n",
        "\n",
        "    print 'already fetched :', count\n",
        "\n",
        "    return rem_list\n",
        "\n",
        "\n",
        "def get_time_left_str( cur_idx, fetch_list, download_pause ):\n",
        "\n",
        "    tweets_left = len(fetch_list) - cur_idx\n",
        "    total_seconds = tweets_left * download_pause\n",
        "\n",
        "    str_hr = int( total_seconds / 3600 )\n",
        "    str_min = int((total_seconds - str_hr*3600) / 60)\n",
        "    str_sec = total_seconds - str_hr*3600 - str_min*60\n",
        "\n",
        "    return '%dh %dm %ds' % (str_hr, str_min, str_sec)\n",
        "\n",
        "\n",
        "def download_tweets( fetch_list, raw_dir ):\n",
        "\n",
        "    # proxy settings for downloading behind a proxy\n",
        "    #os.environ['http_proxy'] = 'http://10.10.78.21:3128/'\n",
        "    #os.environ['https_proxy'] = 'http://10.10.78.21:3128/'\n",
        "\n",
        "    # using python-twitter library\n",
        "    api = twitter.Api(consumer_key='yDkaORxEcwX6SheX6pa1fw',\n",
        "                  consumer_secret='VYIGd2KITohR4ygmHrcyZgV0B74CXi5wsT1eryVtw',\n",
        "                  access_token_key='227846642-8IjK2K32CDFt3682SNOOpnzegAja3TyVpzFOGrQj',\n",
        "                  access_token_secret='L6of20EZdBv48EA2GE8Js6roIfZFnCKBpoPwvBDxF8',\n",
        "                  input_encoding=None, cache=None)\n",
        "\n",
        "    # ensure raw data directory exists\n",
        "    if not os.path.exists( raw_dir ):\n",
        "        os.mkdir( raw_dir )\n",
        "\n",
        "    # stay within rate limits\n",
        "    max_tweets_per_hr  = 180*4\n",
        "    download_pause_sec = 3600 / max_tweets_per_hr\n",
        "\n",
        "    # download tweets\n",
        "    for idx in range(0,len(fetch_list)):\n",
        "\n",
        "        # current item\n",
        "        item = fetch_list[idx]\n",
        "\n",
        "        # print status\n",
        "        trem = get_time_left_str( idx, fetch_list, download_pause_sec )\n",
        "        print '--> downloading tweet #%s (%d of %d) (%s left)' % \\\n",
        "              (item[2], idx+1, len(fetch_list), trem)\n",
        "\n",
        "        # pull data\n",
        "        start = time.time()\n",
        "        try:\n",
        "            tweetStatus = api.GetStatus(item[2])\n",
        "            tweetFile = open(raw_dir + item[2] + '.json', 'w')\n",
        "            tweetFile.write( tweetStatus.AsJsonString() )\n",
        "            tweetFile.close()\n",
        "        except Exception, e:\n",
        "            print 'Cannot download tweet #'+item[2]\n",
        "            print e\n",
        "        end = time.time()\n",
        "\n",
        "        # stay in Twitter API rate limits\n",
        "        print '    pausing %.2f sec to obey Twitter API rate limits' % \\\n",
        "              (download_pause_sec-(end-start))\n",
        "        time.sleep( download_pause_sec-(end-start) )\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def parse_tweet_json( filename ):\n",
        "\n",
        "    # read tweet\n",
        "    print 'opening: ' + filename\n",
        "    fp = open( filename, 'rb' )\n",
        "\n",
        "    # parse json\n",
        "    try:\n",
        "        tweet_json = json.load( fp )\n",
        "    except ValueError:\n",
        "        raise RuntimeError('error parsing json')\n",
        "\n",
        "    # look for twitter api error msgs\n",
        "    if 'error' in tweet_json:\n",
        "        raise RuntimeError('error in downloaded tweet')\n",
        "\n",
        "    # extract creation date and tweet text\n",
        "    return [ tweet_json['created_at'], tweet_json['text'] ]\n",
        "\n",
        "\n",
        "def build_output_corpus( out_filename, raw_dir, total_list ):\n",
        "\n",
        "    # open csv output file\n",
        "    fp = open( out_filename, 'wb' )\n",
        "    writer = csv.writer( fp, delimiter=',', quotechar='\"', escapechar='\\\\',\n",
        "                         quoting=csv.QUOTE_ALL )\n",
        "\n",
        "    # write header row\n",
        "    #writer.writerow( ['Topic','Sentiment','TweetId','TweetDate','TweetText'] )\n",
        "\n",
        "    # parse all downloaded tweets\n",
        "    missing_count = 0\n",
        "    for item in total_list:\n",
        "\n",
        "        # ensure tweet exists\n",
        "        if os.path.exists( raw_dir + item[2] + '.json' ):\n",
        "\n",
        "            try:\n",
        "                # parse tweet\n",
        "                parsed_tweet = parse_tweet_json( raw_dir + item[2] + '.json' )\n",
        "                full_row = item + parsed_tweet\n",
        "\n",
        "                # character encoding for output\n",
        "                for i in range(0,len(full_row)):\n",
        "                    full_row[i] = full_row[i].encode(\"utf-8\").replace('\\n',' ')\n",
        "\n",
        "                # write csv row\n",
        "                writer.writerow( full_row )\n",
        "\n",
        "            except RuntimeError:\n",
        "                print '--> bad data in tweet #' + item[2]\n",
        "                missing_count += 1\n",
        "\n",
        "        else:\n",
        "            print '--> missing tweet #' + item[2]\n",
        "            missing_count += 1\n",
        "\n",
        "    # indicate success\n",
        "    if missing_count == 0:\n",
        "        print '\\nSuccessfully downloaded corpus!'\n",
        "        print 'Output in: ' + out_filename + '\\n'\n",
        "    else:\n",
        "        print '\\nMissing %d of %d tweets!' % (missing_count, len(total_list))\n",
        "        print 'Partial output in: ' + out_filename + '\\n'\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def rebuild_output_corpus():\n",
        "    user_params = {}\n",
        "    user_params['inList'] = './sanderstwitter02/corpus.csv'\n",
        "    user_params['outList'] = './sanderstwitter02/full-corpus.csv'\n",
        "    user_params['rawDir'] = './sanderstwitter02/rawdata/'\n",
        "\n",
        "    total_list = read_total_list( user_params['inList'] )\n",
        "    build_output_corpus( user_params['outList'], user_params['rawDir'],\n",
        "                         total_list )\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # get user parameters\n",
        "    user_params = get_user_params()\n",
        "    dump_user_params( user_params )\n",
        "\n",
        "    # get fetch list\n",
        "    total_list = read_total_list( user_params['inList'] )\n",
        "\n",
        "    # filter out only apple tweets\n",
        "    #total_list = filter_list( total_list )\n",
        "\n",
        "    # pull only 100 tweets\n",
        "    #total_list = random.sample( total_list, 100 )\n",
        "\n",
        "    print 'total tweets : ', len( total_list )\n",
        "    fetch_list = purge_already_fetched( total_list, user_params['rawDir'] )\n",
        "    print 'fetch tweets : ',  len( fetch_list )\n",
        "\n",
        "    # start fetching data from twitter\n",
        "    download_tweets( fetch_list, user_params['rawDir'] )\n",
        "\n",
        "    # second pass for any failed downloads\n",
        "    print '\\nStarting second pass to retry any failed downloads';\n",
        "    fetch_list = purge_already_fetched( total_list, user_params['rawDir'] )\n",
        "    download_tweets( fetch_list, user_params['rawDir'] )\n",
        "\n",
        "    # build output corpus\n",
        "    build_output_corpus( user_params['outList'], user_params['rawDir'],\n",
        "                         total_list )\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}